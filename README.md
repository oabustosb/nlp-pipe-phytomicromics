# nlp-pipe-phytomicromics

__Public repository for the conference article "A zero-shot NLP-based pipeline for automated processing of antimicrobial-related scientific texts".__

The proposed methodology employs a series of Natural Language Processing (NLP) models to facilitate the extraction of useful information from abstracts of scientific papers. The main aims of the process are to determine whether the text is related to the prospecting of antimicrobial compounds, and to identify which those compounds are. To achieve this, a pipeline is implemented, consisting of two stages. First, the texts are passed as input to three large language generation models, that process the texts using a series of questions that determine the acceptance criteria (references to antimicrobial usage, presence of compounds, and usage of compounds in testing). Second, the texts accepted in the first stage are passed to four named entity recognition (NER) models, tuned to find names of chemical compounds. These detected words are compared against the annotations provided by PubTator 3.0 (the PubMed annotation system) to establish the quality of the models.

The experimental setup include the construction of two datasets made of scientific abstracts, one that contains papers actually related to the topic, and others that don't fit into the specific area but share some topics. The positive dataset is divided in two subsets, one for articles fron PubMed Central (PMC) and other for articles from Nature journals. To quantify the quality of the methodology, common metrics for classification and entity recognition are calculated.

The contents of the repository are separated into four different folders, each one corresponding to the stages of the process.

* `original_datasets`: This folder contains three Excel files where the original information on the articles and their abstracts are stored. The required information is only the title and abstract of each article, although some other information can be present but not used. Two of the dataset files conform the positive dataset (i.e., texts that should be approved), and these are `DatasetPositivePmc.xlsx` and `DatasetPositiveNature.xlsx`. The other one contains the negative dataset (texts that should be rejected), and its name is `DatasetNegative.xlsx`.

* `first_stage`: This folder contains the Python code that implements the processing of texts through the use of three text-2-text generation models, that process the texts using three input prompts. In total, each text is processed nine times. Since there are three prompts that corresponds to three criteria of acceptance, the responses for each question are condensed into a single "YES" or "NO" statement, that summarizes the three responses for each question. Finally, a single output is determined, in such a way that the text is accepted when "YES" is the response for all the criteria. This folder contains the Jupyter notebooks in which the code is executed, one for each dataset file: `T2T Dataset PMC.ipynb` and `T2T Dataset Nature.ipynb` for the positive dataset, and `T2T Dataset Negative.ipynb` for the negative dataset. Also, the results of these processes are included, so each dataset file is updated with the nine results of each model, the three results of each criteria, and the final result of the filter process. These files are `Dataset Pmc T2T.xlsx` and `Dataset Nature T2T.xlsx` for the positive dataset, and `Dataset Negative T2T.xlsx` for the negative dataset.

* `second_stage`: This folder contains the Python code that implements the detection of chemical compounds in the texts through the use of four named entity recognition models. Each of these models is tasked independently to annotate all possible chemicals in the text, and their four responses are integrated in such a way that a word or words must appear in at least two of the four responses to be considered as a valid annotation. To establish the quality of the combined annotations, they are compared to the annotations generated by PubTator for the same texts. PubTator annotations are obtained by passing PMIDs (PubMed identifiers) to the PubTator API provided by NCBI. This process is applied only on the positive dataset, and the results of PubTator and the models are compared only for the texts that were accepted in the previous stage. This folder contains three Jupyter notebooks in which the code is executed, two for obtaining the PubTator annotations for the positive texts (one for PMC texts - `PubtatorAnnotationsPmc.ipynb` and one for Nature texts - `PubtatorAnnotationsNature.ipynb`), and another to perform the annotation process using the four NER models - `NamedEntityRecognition.ipynb`. Also, the result of this process is stored as a single Excel file that includes the overall process (both the filter with LLM models and the annotations of PubTator and the NER models) for all positive texts - `DatasetPositiveAll.xlsx`.

* `plots_and_metrics`: This folder contains two Jupyter notebooks that implement two important functions, the visualization of the texts in bot positive and negative datasets, and the calculation of the performance metrics of the process. The file `Scatterplots.ipynb` generates a visualization of the texts in a 2D scatterplot, through a process of vectorization that consists of the preprocessing of the texts, tokenization using Spacy and vectorization with TF-IDF and Latent Semantic Anaysis (LDA). The vectors corresponding to each text are reduced to two-dimensional points with PCA. The file `Metrics.ipynb` measures the performance of the method individually for each stage. The quality of the LLM-based filter (that behaves as a classifier) is measured through the use of multiple classification metrics (accruacy, precision, recall, F1-Score and AUC-ROC). The quality of the chemical compound detection models is assessed through metrics that are better suited in scenarios where NER is used (precision, recall and F1-Score). Some plots with these metrics are also generated.